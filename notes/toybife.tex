\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,hyperref,amssymb,amsmath,amsthm}
\usepackage{algorithmic,xcolor}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{dfn}[thm]{Definition}
% this work...
\newcommand{\al}{\alpha}
\newcommand{\bt}{\beta}
\newcommand{\bal}{{\bm\alpha}}
\newcommand{\eps}{\varepsilon}


\begin{document}

\title{Toy models for validation of Bayesian inference of free energy and path selection}

\author{Alex H. Barnett}
\date{\today}
\maketitle

\begin{abstract}
  We need to be able to test ideas for a Bayesian string method for path selection given cryo particle images.
  We also need to be able to understand why Pilar+Julian's path selection is giving strange results.
  Here are some toy models and their proposed tests.
\end{abstract}

Some standard Bayesian principles:
\ben
\item
A model for what processes create the observations (data)
should be built, then unknowns in this model should be treated as
random variables (if possible), and their posterior is then
prescribed by this same model given the data. The posterior on the unknowns
should be sampled, or fit (via VI), or summarized (its MAP peak found, etc).
\item
Validation should be done from data simulated from the same model,
to ensure that the inference procedure is consistent (the true values
fall within the inferred posterior ranges). This is the
probabilistic
equivalent of testing a numerical algorithm on a known solution.
\een

Note that the idea of extracting a 1D path from a higher-dim conformation
space
already doesn't match principle 1, since the model (higher-dim conformation samples)
differs from that which will be fit (conformations restricted to a 1D path).

{\em To do: consider a fix by allowing a variable transverse covariance matrix to
be fit along with the path---why is this not of interest?}





\bfi[t]
\ig{width=6in}{graphs}
\ca{Probabilistic graphical models for toy and real problems.
  A box with $N$ in the corner indicates iid repeated measurements
  $i=1,\dots,N$. See above for other key, which is standard in PGMs, apparently.
  Note that $G$ and $\rho$ are functions, not just variables.
  (a) Direct observation of conformations. (b) Indirect observation
  of ``images'' (vectors) given by some deterministic function $f(x)$ of the conformation
  plus noise. (c) Cryo-BIFE, similar to (b) but with latent imaging parameters
  ($\theta$ is angles, translations, intensity offsets, etc).
  }{f:graphs}
\efi





% 11111111111111111111111111111111111111111111111111111111111111111111111111111
\section{Warm-up toy problem: direct observation of conformations}

Let $x\in\R^n$ be the conformational space.
If $n=1$ then we have an equivalent problem to inference of a
free energy function $G(x)$ of a single variable $x$ (called $s$ in our
paper), except its simpler because there's no projection down from higher
dimensions.
If $n=2$ (typical for Pilar's group), then $x$ could be $(x,y)$, for instance (CMA,CMB
For general $n$, the model is: $G(x)$, $x\in\Omega$
is an unknown smooth function over some known
region $\Omega \subset \R^n$.
Let $\bt>0$ be the dimensionless inverse temperature.
Conformations $x_i$ are iid sampled from the
Boltzmann (=\,Gibbs) density. Ie, $x_1,\dots, x_N \sim \rho$, where
\be
\rho(x) = Z^{-1} e^{-\bt G(x)}~, \qquad x\in\Omega~,
\label{boltz}
\ee
where $Z := \int_\Omega e^{-\bt G(x)} dx$.
This distribution model basically defines the concept of free energy.
%(including for a $n=1$ path as in our paper).
The goal is to recover $G$ in $\Omega$, up to an additive constant,
equivalent to recovering $\rho$.
We are not trying to restrict things to a path here, just work in the
ambient dimension $n$ that we believe captures the configurations.

The reason we can do this is that in this toy model
we observe the data $D := \{x_i\}$ directly.
A probabilistic graphical model is a useful way to summarize this;
see Fig.~\ref{f:graphs}(a).
Then we have a {\em density estimation} problem for $\rho$,
a standard task in statistics.

\begin{rmk}[density estimation]
There are nonparametric methods: histogram the samples into predetermined bins,
smooth them with kernels, etc.
This needs a lot of samples if we're in $n>1$ dimensions.
There are parametric methods where $\rho$ stays within
a family, whose parameters are inferred (eg, a max-likelihood
point estimate, or Bayesian posterior over parameters).
There is no hard line between parametric and nonparametric:
eg, histogramming may be viewed as a max-likelihood point estimate
for a piecewise-constant parameterization of $\rho$.
Also there are Bayesian kernel density methods (eg Sibisi--Skilling '96).
Todo: read Dirichlet processes.
\end{rmk}

\begin{rmk}[no path optimization]
To be clear, we don't get to test path optimization here, since if conformations
come from $\R^n$, for $n>1$, they are generally inconsistent with any 1D path
without an intermediate noise model as in the next section.
\end{rmk}

The likelihood is $p(D|G) = \prod_i \rho(x_i)$, so its negative log is
\be
-\ln p(D|G) = \sum_i \ln \rho(x_i) = N \ln Z + \bt \sum_i G(x_i)~.
\label{nll}
\ee
Let's parameterize the free energy
\footnote{This is similar to GOLD method from Kathleen Rey '18 thesis, but
  there $G$ is a GP.}
with $p$ fixed basis functions,
in the $n=1$ dim case on $[0,1]$,
\be
G(x) = \sum_{j=1}^p \al_j \phi_j(x)~,\qquad  0\le x \le 1~.
\label{basis}
\ee
Then
for the normalizing constant
we use a $Q$-node quadrature approximation
with nodes $x_q$ and weights $v_q$, $q=1,\dots,Q$,
\bea
Z &= &\int_0^1 e^{-\bt \sum_j \al_j \phi_j(x)} dx
\label{Z1}
\\
&\approx&
\sum_{q=1}^Q v_q e^{-\bt \sum_j \al_j \phi_j(x_q)} = 
\mbf{v}^T \exp (-\bt B \bal)
\eea
where $\mbf{v} := \{v_q\}_{q=1}^Q$
and $\bal := \{\al_j\}_{j=1}^p$
are column vectors, and
$B \in \R^{Q\times p}$ a fixed basis-quadrature matrix with entries
$B_{qj}:= \phi_j(x_q)$. Here exp is taken to act elementwise.

In terms of the coefficient vector $\bal$, then, up to quadrature
error,
\be
- \ln p(D|G) \approx
N \ln [\mbf{v}^T \exp (-\bt B \bal)]
+ \bt \mbf{d}^T \bal
\label{nllbas}
=: L(\bal)
\ee
where $\mbf{d}$ is a data-basis vector with entries
$d_j := \sum_i \phi_j(x_i)$ that needs computing only once.

We want the posterior $\pi(G|D)$.
Assuming a flat prior over $\bal$, the negative log posterior
$U(\bal) := -\log \pi(G|D) = -\log p(D|G) + \mbox{const}$,
which is \eqref{nllbas} up to a constant.
$U$ becomes the potential function in MCMC sampling over $\bal$.
The MAP estimate is $\bal_*:=\arg\min_{\bal\in\R^p} U(\bal)$.
One could sample from the posterior $\pi(\bal):=e^{-U(\bal)}$ by HMC,
then summarize by iid samples or posterior mean, etc.
There will be numerical speedups by exploiting locality of
basis functions, or properties of exp.

{\em 
To do: code up HMC here, using either $\phi_j$ continuous piecewise
linear hat-functions, or something higher-order but still well-conditioned.
Conditioning is crucial for success of MCMC.
Check the posterior quantiles contain the true value the expected fraction
of the time.
}

\begin{rmk}[Additive constant in $G$]
  Although adding $c$ to $G$ multiplies $Z$ by $e^{-\bt c}$, in a predictable way,
  it is in general not true that $G$ having zero mean implies $Z=1$.
  Pilar is right that $Z$ must be computed to perform MCMC correctly.
  \end{rmk}




% 2222222222222222222222222222222222222222222222222222222222222222222222222222
\section{Indirect observations of conformations via noisy ``images'' without
latent parameters}

Here we use ``image'' in the abstract to mean a vector
$w\in \R^P$ derived from a conformation $x$ with a known conditional
pdf $p_\tbox{im}(w|x)$, which is the imaging plus noise model (likelihood).
For an image, $P$ is the number of pixels.
The full model is now $x_i,\dots,x_N \sim \rho$, iid, then
$w_i \sim p_\tbox{im}(\cdot | x_i)$ for each $i$.
Only $w_i$ are observed, not $x_i$, ie the data is $D:=\{w_i\}_{i=1}^N$.

For any additive-noise imaging model we have the likelihood
$$
p_\tbox{im}(w | x) = p_\tbox{noise}(w - f(x))
$$
where $y=f(x)$ is the deterministic {\em forward model} for the image.
$p_\tbox{noise}$ could be a Gaussian with zero mean and variance $\sigma^2$,
another known parameter.
See Fig.~\ref{f:graphs}(b).
Note that additive doesn't cover all cases of $p_\tbox{im}(w|x)$ such as a Poisson noise model.

The toy idea is to make $P$ small, eg, $P=n$ (=1 or 2), and test
consistency for a simple map $f$, which could be the identity map.
This would also allow tests of path-selection methods, where
the 1D path is selected in $\R^2$.
{\em To do: write down a string method where the max likelihood for a given
  set of nodes has its negative log taken to give a ``potential'' for that string. Its grad wrt each node gives a ``force'' on that node. The string has a time-step taken with this force, then is re-interpolated to high order onto an equi-arc-length parameterization, as in the ``improved'' string method paper, or to Chebychev nodes.}

We first derive the posterior $\pi(G|D)$.





%Note we have to set up interpolation off of nodes.


\section{Observation of noisy images which also have latent (nuisance) parameters}

The latent parameters summarized by $\theta$ include:
rotation in SO(3), translation, and any other.
Each particle image $i$ gets a different draw $\theta_i$.
See Fig.~\ref{f:graphs}(c).
This is as in cryo-BIFE.
It has the complication that $\theta$ must be marginalized over.






% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{localrefs}
\end{document}


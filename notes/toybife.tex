\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,hyperref,amssymb,amsmath,amsthm}
\usepackage{algorithmic,xcolor}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{dfn}[thm]{Definition}
% this work...
\newcommand{\al}{\alpha}
\newcommand{\bt}{\beta}
\newcommand{\bal}{{\bm\alpha}}
\newcommand{\eps}{\varepsilon}


\begin{document}

\title{Toy models for validation of Bayesian inference of free energy and path selection}

\author{Alex H. Barnett}
\date{\today}
\maketitle

\begin{abstract}
  We need to be able to test ideas for a Bayesian string method for path selection given cryo particle images.
  We also need to be able to understand why Pilar+Julian's path selection is giving strange results with simulated data (eg, not finding lowest-energy paths).
  To think clearly, one needs to set up toy models, and test that known
  parameters can be inferred correctly by a proposed method.
  Here are some toy models and their proposed tests.
  One conclusion is that, since the generative model has configurations living
  in a higher-dim space, then any attempt to infer using configurations
  restricted to a 1D path will be inconsistent.
  One solution is to change the generative model to only create configurations
  on some 1D path, which differs from the full Gibbs pdf over the full
  configuration space used in Pilar's group's work that I know so far.
\end{abstract}

Some standard Bayesian principles:
\ben
\item
A model for what processes create the observations (data)
should be built, then unknowns in this model should be treated as
random variables (if possible), and their posterior is then
prescribed by this same model given the data. The posterior on the unknowns
should be sampled, or fit (via VI), or summarized (its MAP peak found, etc).
This is called inference on the model given the data.
\item
Validation should be done from data simulated from the same model,
to ensure that the inference procedure is consistent (the true values
fall within the inferred posterior ranges). This is the
probabilistic
equivalent of testing a numerical algorithm on a known solution and
making sure you get the right answer.
\een

Note that the idea of extracting a 1D path from a higher-dim conformation
space
already doesn't match principle 1, since the model (higher-dim conformation samples)
differs from that which will be fit (conformations restricted to a 1D path).

{\em To do: consider a fix by allowing a variable transverse covariance matrix to
be fit along with the path---this would at least be closer to the believed generative model for configurations.}





\bfi[t]
\ig{width=6in}{graphs}
\ca{Probabilistic graphical models for toy and real problems.
  A box with $N$ in the corner indicates iid repeated measurements
  $i=1,\dots,N$. See above for other key, which is standard in PGMs, apparently.
  Note that $G$ and $\rho$ are functions, not just variables.
  (a) Direct observation of conformations. (b) Indirect observation
  of ``images'' (vectors) given by some deterministic function $f(x)$ of the conformation
  plus noise. (c) Cryo-BIFE, similar to (b) but with latent imaging parameters
  ($\theta$ is angles, translations, intensity offsets, etc).
  }{f:graphs}
\efi





% 11111111111111111111111111111111111111111111111111111111111111111111111111111
\section{Warm-up toy problem: direct observation of conformations}

Let $x\in\R^n$ be the conformational space.
If $n=1$ then we have an equivalent problem to inference of a
free energy function $G(x)$ of a single variable $x$ (called $s$ in our
paper), except its simpler because there's no projection down from higher
dimensions.
If $n=2$ (typical for Pilar's group), then $x$ could be $(x,y)$, for instance (CMA,CMB); however there is again no projection down to a 1D path.
For general $n$, the model is: $G(x)$, $x\in\Omega$
is an unknown smooth function over some known
region $\Omega \subseteq \R^n$.
Let $\bt>0$ be the dimensionless inverse temperature.
Conformations $x_i$ are iid sampled from the
Boltzmann (=\,Gibbs) density. Ie, $x_1,\dots, x_N \sim \rho$, where
\be
\rho(x) = Z^{-1} e^{-\bt G(x)}~, \qquad x\in\Omega~,
\label{boltz}
\ee
where $Z := Z_G := \int_\Omega e^{-\bt G(x)} dx$.
This distribution model basically defines the concept of free energy.
%(including for a $n=1$ path as in our paper).
The goal is to recover $G$ in $\Omega$, up to an additive constant,
equivalent to recovering $\rho$.
We are not trying to restrict things to a path here, just work in the
ambient dimension $n$ that we believe captures the configurations.

The reason we can do this is that in this toy model
we observe the data $D := \{x_i\}$ directly.
A probabilistic graphical model is a useful way to summarize this;
see Fig.~\ref{f:graphs}(a).
Then we have a {\em density estimation} problem for $\rho$,
a standard task in statistics.

\begin{rmk}[density estimation]
There are nonparametric methods: histogram the samples into predetermined bins,
smooth them with kernels, etc.
This needs a lot of samples if we're in $n>1$ dimensions.
There are parametric methods where $\rho$ stays within
a family, whose parameters are inferred (eg, a max-likelihood
point estimate, or Bayesian posterior over parameters).
There is no hard line between parametric and nonparametric:
eg, histogramming may be viewed as a max-likelihood point estimate
for a piecewise-constant parameterization of $\rho$.
Also there are Bayesian kernel density methods (eg Sibisi--Skilling '96).
Todo: read Dirichlet processes.
\end{rmk}

\begin{rmk}[no 1D path]
To be clear, we don't get to test fitting $G$ on a 1D path, or doing path optimization here, since if conformations
come from $\R^n$, for $n>1$, they are generally inconsistent with any 1D path
without an intermediate noise model as in the next section.
Pilar and Erik have said
that learning $G$ (ie $\rho$) over a $n=2$ dim or higher
is not of interest like learning it on a 1D path---I am not sure why not.
\end{rmk}

Anyway, let's finish the description of inferring $G$ in the full space.
The likelihood is $p(D|G) = \prod_i \rho(x_i)$, so its negative log is
\be
-\ln p(D|G) = \sum_i \ln \rho(x_i) = N \ln Z + \bt \sum_i G(x_i)~.
\label{nll}
\ee
Let's parameterize the free energy
\footnote{This is similar to GOLD method from Kathleen Rey '18 thesis, but
  there $G$ is a GP.}
with $p$ fixed basis functions,
in the $n=1$ dim case on $[0,1]$,
\be
G(x) = \sum_{j=1}^p \al_j \phi_j(x)~,\qquad  0\le x \le 1~.
\label{basis}
\ee
Then
for the normalizing constant
we use a $Q$-node quadrature approximation
with nodes $x_q$ and weights $v_q$, $q=1,\dots,Q$,
\bea
Z &= &\int_0^1 e^{-\bt \sum_j \al_j \phi_j(x)} dx
\label{Z1}
\\
&\approx&
\sum_{q=1}^Q v_q e^{-\bt \sum_j \al_j \phi_j(x_q)} = 
\mbf{v}^T \exp (-\bt B \bal)
\eea
where $\mbf{v} := \{v_q\}_{q=1}^Q$
and $\bal := \{\al_j\}_{j=1}^p$
are column vectors, and
$B \in \R^{Q\times p}$ a fixed basis-quadrature matrix with entries
$B_{qj}:= \phi_j(x_q)$. Here exp is taken to act elementwise.

In terms of the coefficient vector $\bal$, then, up to quadrature
error,
\be
- \ln p(D|G) \approx
N \ln [\mbf{v}^T \exp (-\bt B \bal)]
+ \bt \mbf{d}^T \bal
\label{nllbas}
=: L(\bal)
\ee
where $\mbf{d}$ is a data-basis vector with entries
$d_j := \sum_i \phi_j(x_i)$ that needs computing only once.

We want the posterior $\pi(G|D)$, more concretely $\pi(\bal|D)$
because one must work in some basis for $G$.
Assuming a flat prior over $\bal$, the negative log posterior
$U(\bal) := -\ln \pi(G|D) = -\ln p(D|G) + \mbox{const}$,
which is \eqref{nllbas} up to a constant.
$U$ becomes the potential function in MCMC sampling over $\bal$.
The MAP estimate is $\bal_*:=\arg\min_{\bal\in\R^p} U(\bal)$.
One could sample from the posterior $\pi(\bal):=e^{-U(\bal)}$ by HMC,
then summarize by iid samples or posterior mean, etc.
There will be numerical speedups by exploiting locality of
basis functions, or properties of exp.

{\em 
To do: code up HMC here, using either $\phi_j$ continuous piecewise
linear hat-functions, or something higher-order but still well-conditioned.
Conditioning is crucial for success of MCMC.
Check the posterior quantiles contain the true value the expected fraction
of the time.
}

\begin{rmk}[Additive constant in $G$]
  Although adding $c$ to $G$ multiplies $Z$ by $e^{-\bt c}$, in a predictable way,
  it is in general not true that $G$ having zero mean implies $Z=1$.
  Pilar is right that $Z$ must be computed to perform MCMC correctly.
  \end{rmk}




% 2222222222222222222222222222222222222222222222222222222222222222222222222222
\section{Indirect observations of conformations via noisy ``images'' without
latent parameters}

Here we use ``image'' in the abstract to mean a vector
$w\in \R^P$ derived from a conformation $x\in\R^n$ with a known conditional
pdf $p_\tbox{im}(w|x)$, which is the imaging plus noise model (likelihood).
For an image, $P$ is the number of pixels.
The generative model is now
\be
x_i,\dots,x_N \sim \rho, \mbox{ iid, over $\R_n$, then }
\qquad w_i \sim p_\tbox{im}(\cdot | x_i), \quad i=1,\dots, N~.
\label{genmod2}
\ee
Only the data $D:=\{w_i\}_{i=1}^N$.
are observed, not the set of conformations $X:=\{x_i\}_{i=1}^N$.

For any additive-noise imaging model we have the likelihood
$$
p_\tbox{im}(w | x) = p_\tbox{noise}(w - f(x))
$$
where $y=f(x)$ is the deterministic {\em forward model} for the image,
given a conformation $x$.
$p_\tbox{noise}$ could be a Gaussian with zero mean and variance $\sigma^2$,
another known parameter.
See Fig.~\ref{f:graphs}(b).
Note that additive doesn't cover all cases of $p_\tbox{im}(w|x)$ such as a Poisson noise model.

The toy idea is to make $P$ small, eg, $P=n$, and test
consistency for a simple map $f$, which could even be the identity map.
This would also allow tests of path-selection methods, where
the 1D path is selected in $\R^2$.

Say we use the {\em same model} for inference
(as in principle 1 at the top),
then we are not allowed to restrict to a 1D path.
We derive the posterior.
The model means the joint pdf factorizes as
\be
p(D,X,G) = p(D|X)p(X|G)p(G)
\label{joint}
\ee
Marginalizing over unobserved configurations $X$ gives
\be
p(D,G) = \int_{x_1\in\R^n}\dots \int_{x_N\in\R^N} p(D,X,G) dx_1\ldots dx_N ~.
\label{marg}
\ee
So, also using the iid nature of \eqref{genmod2}, the posterior is generally
$$
\pi(G|D) \propto p(D,G) = p(D|G)p(G) = p(G) Z^{-N} \prod_i \, \int_{\R^n} p_\tbox{im}(w_i | x_i)
e^{-\bt G(x_i)} dx_i
$$
One could insert a quadrature approximation over $\R^n$
as in the previous section.
Concretely, in the basis \eqref{basis},
the negative log posterior would then be
\be
- \ln p(G|D) \approx
- \ln p(\bal)
+
N \ln [\mbf{v}^T \exp (-\bt B \bal)]
+
\sum_i \ln [\mbf{l}^T_i \exp (-\bt B \bal)]
\label{nlpbas2}
\ee
where for each image $i=1,\dots,N$ the data likelihood at quadrature nodes
vector has entries
\be
\mbf{l}_i := \{v_q p_\tbox{in}(w_i | x_q)\}_{q=1}^Q,
\label{li}
\ee
which needs computing only once (independent of $\bal$).
This big matrix $p_\tbox{in}(w_i | x_q)$ is like a simpler (no-latent variables)
version of $p_\tbox{BioEM}$ of Pilar.

That covers inference for $G$ over $\R^n$ using the true generative model. If instead
we fix a path $x(s)$, $s\in[0,1]$, one could write a posterior
for $G$ (again in the $\bal$ basis) in an {\em incorrect} path model,
which is the same as \eqref{nlpbas2}--\eqref{li}
but with $x_q$ and $v_q$ a quadrature scheme for the 1D path.

Toy test: choose $P=n=2$, $f$ the identity, and
$p_\tbox{noise} \sim {\cal N} (0,\sigma^2 I)$,
so
$$
p_\tbox{im}(w|x) = (2\pi\sigma^2)^{-n/2} e^{-\|w-x\|^2_2/2\sigma^2}
$$
Then ``images'' (data points) $w_i$ are
drawn from Gaussian blobs centered on configs $x_i$.
The big matrix $p_\tbox{in}(w_i | x_q)$ is just the Gaussian pairwise
interactions of data points with (path) quadrature nodes, cheap to fill.
Configs that are further than $\sigma$ from the path have little effect.
I propose this should be tested via MCMC on $\bal$.
If we believe that in BioEM the likelihoods drop off in some Gaussian way
as you move a config away (in $x$ space) from the one that produced a
particular image, then this might be a good model for the full cryo-BIFE
path optimization.
It may address some of the weird behavior
Julian was seeing in cryo-BIFE path optim.

{\em 
As I keep reiterating, there is no ``ground truth'' test possible here
where a $G$ is input and checked to be recovered correctly,
since the path model used for fitting is not the generative model.
One can only check that the $G$ recovered is some $\sigma$-blurred version
of the true $G$ along the path.
}

One solution is to change the generative model to be configs $x$ lying
only on a ground-truth 1D path, with a ground-truth $G$.
Then check that 1) $G$ can be correctly recovered given the correct path,
and 2) without knowing the path, it can be correctly found by optimization
(see below).


\subsection{Path optimization for the path model}

We wish to find a ``force'' vector in $\R^n$ on each quadrature node $q$ on the path.
I propose that the {\em negative log evidence} (NLE) $-\ln p(D)$ for a given
path be treated as a potential energy functional for that path,
and its grad wrt each node be taken to give the forces.
The rest of the improved string method of E et al '07 proceeds as written.
Recall evidence for a model (here, a path and value of $\bt$) is
the predictive density of the data $D$ in the model (ie marginalizing over unknowns, here, $\bal$):
\be
p(D) = \int_{\R^p} p(\bal,D) d\bal = \int_{\R^p} p(D|\bal) p(\bal) d\bal
\ee
This is also the normalizing denominator for the posterior.
As usual we write $\bal$ and $G$ interchangeably.

However, the NLE isn't always easy to calculate numerically.
In the Laplacian approximation it is: say the posterior $p(\bal|D)$ is
nearly Gaussian, then its integral is well approximated by the
desntiy at the MAP times the volume factor coming from the det of the
Hessian of $-\ln p(\bal|D)$.
That would be saying that $G$ is well-specified by the data enough to
be in a linear regime.
A detail is that both these factors have to have their grad taken,
and it's not obvious how to take the grad of a max (the MAP)---this might
involve a Legendre xform or something.

As a cruder variant to warm up with, just use the peak of joint density
$$
p(\bal^\ast,D) = p(D|\bal^\ast) p(\bal^\ast)
$$
in place of the evidence $p(D)$.
For flat prior, this is up to a constant
just the max likelihood value $p(D|G^\ast)$.
This is what I've suggested to Pilar's group in the full cryo-BIFE setting.

To make a correct validation test, changing the generative
model to be on some ground-truth path, and seeing if that path
can be recovered, and the correct $G$ on it, seems to be needed.


To finish.

{\em To do: write down a string method where the max likelihood for a given
  set of nodes has its negative log taken to give a ``potential'' for that string. Its grad wrt each node gives a ``force'' on that node. The string has a time-step taken with this force, then is re-interpolated to high order onto an equi-arc-length parameterization, as in the ``improved'' string method paper of E et al '07, or to Chebychev nodes.}



\section{Observation of noisy images which also have latent (nuisance) parameters}


The latent parameters summarized by $\theta$ include:
rotation in SO(3), translation, imaging system unknowns, etc.
Each particle image $i$ gets a different draw $\theta_i$.
The point really was really to draw Fig.~\ref{f:graphs}(c)
and compare its structure to the simpler toy models.
This is as in cryo-BIFE.
It has the complication that $\theta$ must be marginalized over
when filling the big matrix $p_\tbox{in}(w_i | x_q)$, which is now
$p_\tbox{BioEM}$ of Pilar.

To test, a toy set of latent parameters
could be used in a slight variant of the toy case from section 2.

To do.

\subsection{Path optimization for the path model}

To do.

In order to prevent the multi-hour computation of $p_\tbox{BioEM}$ each
time the path is changed, we have to precompute it to a set of $x$ on a
grid covering $\Omega \subset \R^n$,
interpolate the log likelihoods from this grid.




% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{localrefs}
\end{document}

